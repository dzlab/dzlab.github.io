---
layout: post
comments: true
title: Scale LLM-based applications to millions with LangChain and GPTCache
excerpt: Learn how to leverage GPTCache with LangChain to reduce latency of LLM-based applications and improve scalability
tags: [nlp,langchain]
toc: true
img_excerpt:
---

## Overview
In Software Engineering, whenever there is high cost for producing a result for a given query, a cache is used to avoid wasting resources again and again on calcuting the same result. Usually, the way a cache is a key-value data structure used as follows:
- For a first time seen query, the results are stored temporarily in high-speed storage layers (e.g. RAM or SSDs), 
- When a new query arrives, we first check if results are available in the cache before triggering a new caculation
- Results are sent back to the client and also store in the cache for next time retrieval

The use of cache, in most cases, causes application performance boost, better scalability, and reduced operational and financial costs (see [OpenAI API pricing](https://openai.com/pricing)).

In the case of LLM applications, caching usually relies on the use of embedding algorithms to convert queries into embeddings and then uses a vector store for similarity search on these embeddings. This allows the identification and retrieval of similar prompt/queries from the cache so that answers are returned immediately without calling model endpoints.

### Enter GPTCache
The LangChain library has become the backbone of LLM-based applications, it simplifies the development a lot and allows the chaining (hence the name) of different components: streamline prompt optimization, invoke models API, etc. It does provide serveral ways to cache prompt-completion pairs via third-party integrations. [GPTCache](https://zilliz.com/what-is-gptcache) is one of the well supported LLM cache systems.

<img alt="infrastructure related to GPTCache" src="https://zilliz.com/images/opensourceGptCache/infra.svg">

As depicted in the above diagram, GPTCache has several modules:

- **LLM Adapter** allows a smooth integratation with with LLMs
- **Multimodal Adapter** allows the integratation with multimodal models
- **Embedding Generator** allows the use several embedding algorthms such as OpenAI embeddings
- **Cache Storage** to save LLM responses
- **Vector Store** supports vectordbs Milvus, FAISS and Chroma among others
- **Cache Manager** implements different eviction strategies to ensure the cache is clean and not full
- **Similarity Evaluator** collects data from Cache and Vector Storage and evaluates the similarity between the input request and stored embeddings


### LLM-based application 

- Gain a good understanding of caching's role in mitigating high operational costs, improving response times, and managing network traffic
- Download and process 58-page Arxiv paper for hands-on testing
- Learn to create embeddings and store them in 
@milvusio
, a powerful open-source vector database for AI applications
- Get acquainted with GPTCache, a semantic cache built to store LLM responses effectively
- Boost the performance and scalability of your 
@LangChainAI
 applications


In the remaining of this article, we will see how caching the responses generated by language models improves the efficiency and speed of LLM-based applications. 

## Setup

Curious about how your 
@LangChainAI
 app can scale to millions of users without missing a beat?üåç

Smart implementation of the cache is one of the key aspects!




### LLM


Generate OPENAI API Key, then create a file named '.env', and define it
```shell
OPENAI_API_KEY=<your_key_here>
```

### Instalation


Let's continue!

Pre-requisites to have in place before starting:

1. Having Python3 installed
2. Basic familiarity with command-line interfaces
3. Having permissions to install libraries/packages on their system.

Nice, let's start by running the following commands in your terminal to create and activate a virtual environment

```shell
mkdir langchain-caching && cd langchain-caching
python3 -m venv langchain-caching-env
source langchain-caching/bin/activate
```

First, install all required libraries/packages
```shell
pip3 install langchain gptcache openai tiktoken python-dotenv ipykernel jupyter
pip3 install arxiv pypdf
```

We are ready to start, let's import required libraries
```python
# Imports
from urllib.error import HTTPError
from dotenv import load_dotenv
from tqdm import tqdm

import logging
import arxiv
import time

# recursively tries to split by different characters to find one that works
from langchain.text_splitter import RecursiveCharacterTextSplitter
# loads pdfs from a given directory
from langchain.document_loaders import PyPDFDirectoryLoader

from langchain import OpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Milvus

from gptcache.adapter.langchain_models import LangChainLLMs
from gptcache import cache
from gptcache.embedding import Onnx
from gptcache.manager import CacheBase, VectorBase, get_data_manager
from gptcache.similarity_evaluation.distance import SearchDistanceEvaluation

# loads env variables
load_dotenv()
# to inspect network behavior and API logic of Arxiv and Chroma
```

### Vector Databse
You will use Milvus for caching, and you need Docker Compose to run it locally.

Follow the instructions for your OS here and verify that it works

docs.docker.com

Let's get the YAML file to run Milvus, run the following command in your terminal

```shell
curl https://github.com/milvus-io/milvus/releases/download/v2.2.10/milvus-standalone-docker-compose.yml -o docker-compose.yml
```

Start Milvus

```shell
docker-compose up -d
```

You should see that the containers are running, if you run "docker ps" in your terminal


### GPTCache
In order to work with GPTCache, you have to initialize it first
```python
def get_content_func(data, **_):
    return data.get("prompt").split("Question")[-1]

onnx = Onnx()
cache_base = CacheBase("sqlite")
vector_base = VectorBase(
    "milvus",
    host="127.0.0.1", port="19530",
    dimension=onnx.dimension,
    collection_name="arxiv_paper"
    )
data_manager = get_data_manager(cache_base, vector_base)
cache.init(
    pre_embedding_func=get_content_func,
    embedding_func=onnx.to_embeddings,
    data_manager=data_manager,
    similarity_evaluation=SearchDistanceEvaluation(),
)
cache.set_openai_key()
```

## Knowledge base
Let's search Arxiv paper first, you search for the paper with provided ID
```python
search = arxiv.Search(
    query = "2303.18223" # ID of the paper A survey of Large Language Models
)
```

Let's have a look at the metadata of the downloaded paper
```python
for result in search.results():
    print(f"    Link: {result.pdf_url}")
    print(f"      ID: {result.get_short_id()}")
    print(f"   Title: {result.title}")
    print(f"Category: {result.categories}")
    print(f" Summary: {result.summary[:200]}")
```

Create a directory and download the paper
```python
!mkdir arxiv_papers
dirpath = "arxiv_papers"
```

```python
for paper in tqdm(search.results()):
    paper.download_pdf(dirpath=dirpath)
    print(f"Paper ID {paper.get_short_id()} with title '{paper.title}' is downloaded.")
```

Load all the pages from the paper
```python
papers = []
loader = PyPDFDirectoryLoader(dirpath)
pages = loader.load()

print(f"Total number of pages loaded: {len(pages)}")
```

You will merge all pages into a single text block for splitting. Then you can recursively split the text with chunk_size 1000and chunk_overlap 0, play with these parameters to evaluate the response quality later.
```python
full_text = ''.join([page.page_content for page in pages])

full_text = " ".join(line for line in full_text.splitlines() if line)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.create_documents([full_text])
```

You can now store the embeddings in Milvus
```python
embeddings = OpenAIEmbeddings()

vector_db = Milvus.from_documents(docs, embeddings, connection_args={"host": "localhost", "port": "19530"})
```

Let's ask a simple question to see what we retrieve from Milvus

```python
query = "What are the recent advancements in LLMs?"
docs = vector_db.similarity_search(query)
```

Because we enabled logging, we will see DEBUG messages that provides information on how request is routed to OpenAI API, which data is sent, and what is the response.

## Querying the Knowledge base

Now, you can ask the same question to generate the response with provided context that is created from retrieved documents

```python
llm = LangChainLLMs(llm=OpenAI(temperature=0))
chain = load_qa_chain(llm, chain_type="stuff")
res = chain.run(input_documents=docs, question=query)
print(res)
```

At this point, your question and response pair is cached, but here's the nice part. The response will be re-used if a user submits a similar prompt like "Tell me about the recent advancements in LLMs". You will receive the same answer and you can see the cached question.

```python
query = "Tell me about the recent advancements in LLMs?"
res = chain.run(input_documents=docs, question=query)
print(res)
```

Let's ask another question
```python
query = "Are LLMs able to solve various legal tasks?"
res = chain.run(input_documents=docs, question=query)
print(res)
```

And again a similar question after
```python
query = "Do LLMs have abilities of legal interpretation and reasoning?"
res = chain.run(input_documents=docs, question=query)
print(res)
```

The question "Are LLMs able to solve various legal tasks?" was not semantically similar to the ones that were cached, so there was a new request made, but the question ""Do LLMs have abilities of legal interpretation and reasoning?" was semantically similar to the cached ones, so response from cache is returned.

Hope this walk-through was helpful for you to understand the power of semantic caches. When you scale your application to 10x, 100x, 1000x users, caching strategies across your application components will be extremely important for you to stay efficient and performant.



## That's all folks
I hope you enjoyed this article, feel free to leave a comment or reach out on twitter¬†[@bachiirc](https://twitter.com/bachiirc).
