---
layout: post
comments: true
title: Run LLMs from Hugging Face on GCP with Cloud Run and Cloud Storage
excerpt: How to run custom LLMs on GCP with Cloud Run and how to use Cloud Storage to host the weights downloaded from Hugging Face Hub
tags: [ai,gcp,genai]
toc: true
img_excerpt:
---

![GCP LLMs architecture]({{ "/assets/2023/08/20230820-gcp-huggingface.svg" | absolute_url }})

This article demonstrates how to run a custom Large Language Model on GCP with Cloud Run and use Cloud Storage as a network file system to host the weights downloaded from a model hub. By leaveraging Cloud Storage, the weights will be downloaded once so we can scale the number of Cloud Run containers up or down faster as new instances will not have to pay extra time for downloading model weights again and again. 

We will see how to mount a Cloud Storage bucket onto our Cloud Run container using the open source [FUSE](http://fuse.sourceforge.net/) adapter to share data between multiple containers and services.

> Note: To mount a file system, we need to use the Cloud Run [2nd generation execution environment](https://cloud.google.com/run/docs/about-execution-environments).

## Design Overview
The diagram shows the Cloud Run service connecting to the Cloud Storage bucket via the gcsfuse FUSE adapter. The Cloud Run service and Cloud Storage bucket are located within same region to remove networking cost and for best performance.


Objectives
Create a Cloud Storage bucket to serve as a file share.

Build a Dockerfile with system packages and init-process to manage the mount and application processes.

Deploy to Cloud Run and verify access to the file system in the service.


## Infrastructure setup

The above diagram illustrates a high level architecture for our serverless meeting minutes generator application.



#### Cloud Storage
Create a Cloud Storage bucket or reuse an existing bucket:
```shell
gsutil mb -l REGION gs://BUCKET_NAME
```

Create a service account to serve as the service identity:
```shell
gcloud iam service-accounts create fs-identity
```

Grant the service account access to the Cloud Storage bucket:
```shell
gcloud projects add-iam-policy-binding PROJECT_ID \
     --member "serviceAccount:fs-identity@PROJECT_ID.iam.gserviceaccount.com" \
     --role "roles/storage.objectAdmin"
```

#### Defining your environment configuration with the Dockerfile
This Cloud Run service requires one or more additional system packages not available by default. The RUN instruction will install tini as our init-process and gcsfuse, the FUSE adapter. Read more about working with system packages in your Cloud Run service in the Using system packages tutorial.

The next set of instructions create a working directory, copy source code, and install app dependencies.

The ENTRYPOINT specifies the init-process binary that prepends to the CMD instructions, in this case it's the startup script. This launches a single tini process and then proxies all received signals to a session rooted at that child process.

The CMD instruction sets the command to be executed when running the image, the startup script. It also provides default arguments for the ENTRYPOINT. Understand how CMD and ENTRYPOINT interact.

```Dockerfile
# Use the official lightweight Python image.
# https://hub.docker.com/_/python
FROM python:3.11-buster

# Install system dependencies
RUN set -e; \
    apt-get update -y && apt-get install -y \
    tini \
    lsb-release; \
    gcsFuseRepo=gcsfuse-`lsb_release -c -s`; \
    echo "deb http://packages.cloud.google.com/apt $gcsFuseRepo main" | \
    tee /etc/apt/sources.list.d/gcsfuse.list; \
    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | \
    apt-key add -; \
    apt-get update; \
    apt-get install -y gcsfuse \
    && apt-get clean

# Set fallback mount directory
ENV MNT_DIR /mnt/gcs

# Copy local code to the container image.
ENV APP_HOME /app
WORKDIR $APP_HOME
COPY . ./

# Install production dependencies.
RUN pip install -r requirements.txt

# Ensure the script is executable
RUN chmod +x /app/entrypoint.sh

# Use tini to manage zombie processes and signal forwarding
# https://github.com/krallin/tini
ENTRYPOINT ["/usr/bin/tini", "--"] 

# Pass the startup script as arguments to Tini
CMD ["$APP_HOME/entrypoint.sh"]
```


#### Defining your processes in the startup script

The startup script creates the mount point directory, where the Cloud Storage bucket will be made accessible. Next the script attaches the Cloud Storage bucket to the service's mount point using the gcsfuse command, then starts the application server. The gcsfuse command has built-in retry functionality; therefore further bash scripting is not necessary. Lastly, the wait command is used to listen for any background processes to exit then exits the script.

```shell
#!/usr/bin/env bash
set -eo pipefail

# Create mount directory for service
mkdir -p $MNT_DIR

echo "Mounting GCS Fuse."
gcsfuse --debug_gcs --debug_fuse $BUCKET $MNT_DIR 
echo "Mounting completed."

# Create directory for Hugging Face 
mkdir -p $MNT_DIR/hf

# Export needed environment variables
export HF_HOME=$MNT_DIR/hf
export HF_HUB_ENABLE_HF_TRANSFER=1
export SAFETENSORS_FAST_GPU=1
export BITSANDBYTES_NOWELCOME=1
export PIP_DISABLE_PIP_VERSION_CHECK=1
export PIP_NO_CACHE_DIR=1

# Download model weights
echo "Downloading from Hugging Face Hub."
$APP_HOME/download.sh
echo "Downloading completed."

# Run the web service on container startup. Here we use the gunicorn
# webserver, with one worker process and 8 threads.
# For environments with multiple CPU cores, increase the number of workers
# to be equal to the cores available.
# Timeout is set to 0 to disable the timeouts of the workers to allow Cloud Run to handle instance scaling.
python3 $APP_HOME/main.py
```


In the same directory as the previous file `entrypoint.sh`, define a `requirements.txt` file to declare all of our dependencies:

```
pytorch-cuda=11.7
google-cloud-storage
transformers~=4.28.1
safetensors~=0.3.0
accelerate~=0.18.0
bitsandbytes~=0.38.1
sentencepiece~=0.1.98
hf-transfer~=0.1.3
msgspec~=0.14.2
```

This is the content of `download.sh`, it will download the model weights (in this case `google/flan-t5-base`)

```python
#!/usr/bin/env python3

import torch
from huggingface_hub import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = snapshot_download("google/flan-t5-base", ignore_patterns=["*.md"])
m = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    local_files_only=True,
    )
m.save_pretrained(model_path, safe_serialization=True, max_shard_size="24GB")

tok = AutoTokenizer.from_pretrained(model_path)
tok.save_pretrained(model_path)
[p.unlink() for p in Path(model_path).rglob("*.bin")]  # type: ignore
```

`model.py`

```python
import torch
from transformers import AutoTokenizer, TextIteratorStreamer, pipeline

class TextGenerationLLM:
    def __init__(self, model_url: str = "google/flan-t5-base"):
        # make sure we don't connect to HF Hub
        os.environ["HF_HUB_OFFLINE"] = "1"
        os.environ["TRANSFORMERS_OFFLINE"] = "1"
        # setup text generation pipeline
        tokenizer = AutoTokenizer.from_pretrained(model_url, local_files_only=True)
        self.generator = pipeline(
            "text-generation",
            model=model_url,
            tokenizer=tokenizer,
            torch_dtype=torch.float16,
            device_map="auto",
            model_kwargs={"local_files_only": True},
        )
        self.generator.model = torch.compile(self.generator.model)

    def generate(self, prompt: str) -> str:
        results = generator(prompt, max_length=1000, do_sample=True)
        return results[0]["generated_text"]
```

`main.py` file

```python
from model import TextGenerationLLM
from flask import Flask, request

llm = TextGenerationLLM()

@app.route('/predict', methods= ['POST'])
def predict():
    prompt = request.data
    response = llm.predict(prompt)
    return response

if __name__ == "__main__":
    app.run(port=8000, host='0.0.0.0', debug=True)
```

#### Build and deploy the container image to Cloud Run:
```shell
gcloud run deploy llm-run --source . \
    --execution-environment gen2 \
    --allow-unauthenticated \
    --service-account fs-identity \
    --update-env-vars BUCKET=BUCKET_NAME
```


## That's all folks

Using a network file system with Cloud Run requires advanced Docker knowledge because your container must run multiple processes, including the file system mount and application process. This tutorial explains the necessary concepts alongside a working example; however, as you adapt this tutorial to your own application, make sure you understand the implications of any changes you might make.


https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/stable_lm/main.py

https://cloud.google.com/run/docs/tutorials/network-filesystems-fuse


https://cloud.google.com/filestore/docs/mounting-fileshares


In this article we saw how easy it is to use Google Cloud to build innovative and scalable applications. We used the following services from GCP:
- Cloud Storage to store audio files and
- PubSub to react to events such us when a new audio file is uploaded to Cloud Storage. 
- Cloud Functions used to process meeting recording files and generating the minutes.
- Two foundation models from Vertex AI: Chirp for speech to text and PaLM for text generation.

I hope you enjoyed this article, feel free to leave a comment or reach out on twitterÂ [@bachiirc](https://twitter.com/bachiirc).
